from typing import List
import re

class Tokenizer:
    def __init__(self, join_verbs: bool = True):
        self.join_verbs = join_verbs
        self.separators = r'[\t \xa0\n]'
        self.before_verbs = [
            "می",
            "خواهم",
            "خواهی",
            "خواهد",
            "خواهیم",
            "خواهید",
            "خواهند",
            "نخواهم",
            "نخواهی",
            "نخواهد",
            "نخواهیم",
            "نخواهید",
            "نخواهند",
        ]
        self.after_verbs = [
            "ام",
            "ای",
            "است",
            "ایم",
            "اید",
            "اند",
            "بودم",
            "بودی",
            "بود",
            "بودیم",
            "بودید",
            "بودند",
            "باشم",
            "باشی",
            "باشد",
            "باشیم",
            "باشید",
            "باشند",
            "شده_ام",
            "شده_ای",
            "شده_است",
            "شده_ایم",
            "شده_اید",
            "شده_اند",
            "شده_بودم",
            "شده_بودی",
            "شده_بود",
            "شده_بودیم",
            "شده_بودید",
            "شده_بودند",
            "شده_باشم",
            "شده_باشی",
            "شده_باشد",
            "شده_باشیم",
            "شده_باشید",
            "شده_باشند",
            "نشده_ام",
            "نشده_ای",
            "نشده_است",
            "نشده_ایم",
            "نشده_اید",
            "نشده_اند",
            "نشده_بودم",
            "نشده_بودی",
            "نشده_بود",
            "نشده_بودیم",
            "نشده_بودید",
            "نشده_بودند",
            "نشده_باشم",
            "نشده_باشی",
            "نشده_باشد",
            "نشده_باشیم",
            "نشده_باشید",
            "نشده_باشند",
            "شوم",
            "شوی",
            "شود",
            "شویم",
            "شوید",
            "شوند",
            "شدم",
            "شدی",
            "شد",
            "شدیم",
            "شدید",
            "شدند",
            "نشوم",
            "نشوی",
            "نشود",
            "نشویم",
            "نشوید",
            "نشوند",
            "نشدم",
            "نشدی",
            "نشد",
            "نشدیم",
            "نشدید",
            "نشدند",
            "می‌شوم",
            "می‌شوی",
            "می‌شود",
            "می‌شویم",
            "می‌شوید",
            "می‌شوند",
            "می‌شدم",
            "می‌شدی",
            "می‌شد",
            "می‌شدیم",
            "می‌شدید",
            "می‌شدند",
            "نمی‌شوم",
            "نمی‌شوی",
            "نمی‌شود",
            "نمی‌شویم",
            "نمی‌شوید",
            "نمی‌شوند",
            "نمی‌شدم",
            "نمی‌شدی",
            "نمی‌شد",
            "نمی‌شدیم",
            "نمی‌شدید",
            "نمی‌شدند",
            "خواهم_شد",
            "خواهی_شد",
            "خواهد_شد",
            "خواهیم_شد",
            "خواهید_شد",
            "خواهند_شد",
            "نخواهم_شد",
            "نخواهی_شد",
            "نخواهد_شد",
            "نخواهیم_شد",
            "نخواهید_شد",
            "نخواهند_شد",
        ]

    def tokenize(self, text: str) -> List[str]:
        tokens = [word for word in re.split(self.separators, text)]

        if self.join_verbs:
            tokens = self.join_verb_parts(tokens)

        return tokens

    def join_verb_parts(self, tokens: List[str]) -> List[str]:
        result = [""]
        for token in reversed(tokens):
            if token in self.before_verbs or (
                    result[-1] in self.after_verbs
            ):
                result[-1] = token + "_" + result[-1]
            else:
                result.append(token)
        return list(reversed(result[1:]))
